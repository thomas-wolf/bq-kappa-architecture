{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "849bd930-734a-4fd9-92ac-2c531908c1de",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Kappa Architecture\n",
    "\n",
    "This notebook illustrates how to setup a Kappa Architecture for streaming tables. Its purpose is to demonstrate the implementation of Kappa Architecture. For details about Kappa Architecture, see #TODO Add link to article.\n",
    "\n",
    "For this demo, we will use a (very simple) customer object. We will generate the required tables and views and generate sample data. The customer structure we are going to use looks as follows: \n",
    "\n",
    "| customer_id | first_name | last_name | email_permission | ingest_ts               |\n",
    "|------|---------|-----|--------|-------------------------|\n",
    "| 1   | John-Fitz | Doe | true   | 2021-12-07 17:42:12 UTC |\n",
    "| 2   | Jane    | Doe |true   | 2021-12-17 09:15:12 UTC|\n",
    "\n",
    "\n",
    "customer_id is our key which helps us with identifying customers uniquely. Multiple records sharing the same customer_id are considered being updates. \n",
    "First name and last name are self explanatory. Email permission indicates whether the customer gave their consent to be contacted by email.\n",
    "Ingest TS denotes the point in time at which the record has been written into the BigQuery table.\n",
    "\n",
    "# Settings & Customization\n",
    "\n",
    "You can change the default naming options in the first cell in the table_names dictionary:\n",
    "\n",
    "* streaming_table: The table into which the data is ingested into in a near-realtime append only manner\n",
    "* latest_version_view: Name of the view that calculates the latest version for each entity based on the streaming table\n",
    "* latest_table: Table that only contains the latest version per entity\n",
    "* kappa_view: Kappa view: Gives you back the same results as the latest_version_view (only the latest version using all available data, not only what has been ingested to the latest table already) in an efficient manner. Hopefully consumes little data than the latest_version_view for doing so (quot est demonstrandum)\n",
    "\n",
    "**All tables and views will be created with default expiration time 24h, so there is no need to clean up after running this notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6061c303-878c-414f-9704-2c5e5c860a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (0.70.14)\n",
      "Requirement already satisfied: dill>=0.3.6 in /opt/conda/lib/python3.7/site-packages (from multiprocess) (0.3.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ipython-bg in /opt/conda/lib/python3.7/site-packages (0.2)\n",
      "Requirement already satisfied: ipython in /opt/conda/lib/python3.7/site-packages (from ipython-bg) (7.34.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython->ipython-bg) (0.7.5)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipython->ipython-bg) (5.2.1.post0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython->ipython-bg) (5.1.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.7/site-packages (from ipython->ipython-bg) (0.1.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.7/site-packages (from ipython->ipython-bg) (4.8.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.7/site-packages (from ipython->ipython-bg) (0.18.1)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython->ipython-bg) (0.2.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython->ipython-bg) (2.12.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython->ipython-bg) (3.0.29)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.7/site-packages (from ipython->ipython-bg) (59.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.16->ipython->ipython-bg) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect>4.3->ipython->ipython-bg) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->ipython-bg) (0.2.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Some initial setup ...\n",
    "%pip install multiprocess  # need to use multiprocess instead of builtin multiprocessing due to running in an interactive shell\n",
    "%pip install ipython-bg\n",
    "%load_ext ipython_bg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bbbaeaa-fb29-46d7-af9a-73469314ffa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customize table / dataset names here \n",
    "\n",
    "import datetime\n",
    "from google.cloud import bigquery\n",
    "\n",
    "bq = bigquery.Client(location=\"EU\")   # MMS will not use other then EU locations - change accordingly\n",
    "dataset_name = \"demo_kappa_architecture_final_2\"  # BQ dataset name\n",
    "dataset_default_table_expiration_ms = 24 * 60 * 60 * 1000 # Tables in this dataset will expire after this amount of ms (24h)\n",
    "\n",
    "\n",
    "\n",
    "table_names = { \"streaming_table\": \"customers_stream\"   # Table names for streaming & latest table and kappa view\n",
    "               , \"latest_table\": \"customers_latest\"\n",
    "               , \"kappa_view\": \"v_customers_latest\" \n",
    "               , \"latest_version_view\": \"v_customers_streaming_latest\"\n",
    "              }\n",
    "\n",
    "target_tables = [f\"{dataset_name}.{table_names['streaming_table']}\"]\n",
    "\n",
    "# Create dataset and configure expiration of tables\n",
    "dataset_ref = bq.dataset(dataset_name)    \n",
    "dataset = bq.create_dataset(dataset_ref,exists_ok=True)\n",
    "dataset.default_table_expiration_ms = dataset_default_table_expiration_ms\n",
    "\n",
    "dataset = bq.update_dataset(\n",
    "    dataset, [\"default_table_expiration_ms\"]\n",
    ") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27ffee2f-7717-41ba-9b98-f615a5eb5105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_streaming_table():\n",
    "    \"\"\"\n",
    "    Creates our base table into which we stream our customer data\n",
    "    \"\"\"\n",
    "    partitioned_by_day = f\"\"\"\n",
    "    CREATE  TABLE IF NOT EXISTS {dataset_name}.{table_names['streaming_table']}\n",
    "    ( customer_id String\n",
    "     , first_name String\n",
    "     , last_name String\n",
    "     , email_permission Boolean \n",
    "     , ingest_ts Timestamp\n",
    "     )\n",
    "     PARTITION BY TIMESTAMP_TRUNC(ingest_ts,DAY)\n",
    "     CLUSTER BY customer_id    # without clustering, it's not going to work\n",
    "    \"\"\"\n",
    "    \n",
    "    bq.query(partitioned_by_day).result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e65e146a-603e-4e4a-a78e-4a45140a0ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methods for generating sample data\n",
    "\n",
    "import random,uuid\n",
    "from enum import Enum\n",
    "import names\n",
    "from datetime import timedelta\n",
    "\n",
    "class Gender(Enum):\n",
    "    FEMALE = 'female'\n",
    "    MALE = 'male'\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.value)\n",
    "\n",
    "def get_rnd_boolean(prob_true=0.8):\n",
    "    rnd = random.random()\n",
    "    if rnd > prob_true:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def get_rnd_ingest_ts(max_days_back=15,max_hours_back=24,max_minutes_back=60):\n",
    "    \"\"\"\n",
    "    returns a random ingest_ts\n",
    "    \"\"\"\n",
    "    ingest_ts = datetime.datetime.utcnow() - timedelta(days=random.randint(0,max_days_back),hours=random.randint(0,max_hours_back),minutes=random.randint(0,max_minutes_back))\n",
    "\n",
    "    return ingest_ts\n",
    "\n",
    "from functools import lru_cache\n",
    "@lru_cache(maxsize=1)\n",
    "def get_customer_ids(n=1000):\n",
    "    \"\"\"\n",
    "    Gets us some IDs of already existing customers. Can be used for generating updates to existing customers. The LRU cache annoation prevents the function\n",
    "    from querying BQ each time, but takes away the sampling feature if called with same argument. Good enough for demo purposes.\n",
    "    \"\"\"\n",
    "    query = f\"\"\"SELECT customer_id from {dataset_name}.{table_names['streaming_table']}\n",
    "             WHERE DATE(ingest_ts) < CURRENT_DATE()\n",
    "             AND  rand() < 0.1 LIMIT {n} \"\"\"\n",
    "    \n",
    "    result = bq.query(query)\n",
    "    return result.to_dataframe().iloc[:, 0].to_list()\n",
    "        \n",
    "    \n",
    "def get_random_customer(for_streaming=False,update_rate=0.01,known_customers:list=None):\n",
    "    \"\"\"\n",
    "    Generates us a random customers. If you pass in a list of known customers and set update_rate > 0, the share of update rate percent records will be updates instead of new\n",
    "    customer ids.\n",
    "    \"\"\"\n",
    "    \n",
    "    gender = random.choice(list(Gender))\n",
    "\n",
    "    # For the initial load, we also need to generate older data\n",
    "    max_days_back = 15            \n",
    "    max_hours_back = 24\n",
    "    max_minutes_back = 60\n",
    "    \n",
    "    # For simulation of near-realtime streaming, we only create recent data \n",
    "    if for_streaming:\n",
    "        max_days_back = 0\n",
    "        max_hours_back = 0\n",
    "        max_minutes_back = 30\n",
    "\n",
    "    rnd_ts = get_rnd_ingest_ts(max_days_back,max_hours_back,max_minutes_back)\n",
    "    \n",
    "    customer = { \"customer_id\":  str(uuid.uuid4())\n",
    "           , \"first_name\": names.get_first_name(gender=gender)\n",
    "           , \"last_name\": names.get_last_name()\n",
    "           , \"email_permission\": get_rnd_boolean()\n",
    "            , \"ingest_ts\": rnd_ts.isoformat()\n",
    "           }\n",
    "    \n",
    "    if known_customers and random.random() < update_rate:\n",
    "        customer[\"customer_id\"] = random.choice(known_customers)\n",
    "        \n",
    "    return customer\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "def get_random_customers(n=1000, jobs=5, **kwargs ):\n",
    "    \"\"\"Wrapper that helps us generating a bigger number of customers\"\"\"\n",
    "    return Parallel(n_jobs=jobs)(delayed(get_random_customer)(**kwargs) for i in range(n))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d378db6-bcd6-446a-89eb-f5610c143451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def streaming_insert(data,table):\n",
    "    \"\"\"\n",
    "    Does a streaming insert to BQ. Uses the deprecated insert_rows method for simplicity. Use BQ Storage API Write instead!\n",
    "    \"\"\"\n",
    "    \n",
    "    table = bq.get_table(table)\n",
    "    errors = bq.insert_rows(table,data)\n",
    "    return errors\n",
    "\n",
    "def multiply_records_in_table(table,factor=1):\n",
    "    \"\"\"Multiplies records in the table by just copying it factor number of times\"\"\"\n",
    "    \n",
    "    sql = f\"\"\"\n",
    "    INSERT INTO {dataset_name}.{table}\n",
    "    with records AS (\n",
    "     SELECT * FROM {dataset_name}.{table}\n",
    "     )\n",
    "    , multiplier as (\n",
    "      SELECT 1 from {dataset_name}.INFORMATION_SCHEMA.PARTITIONS LIMIT {factor}\n",
    "      )\n",
    "    SELECT r.* FROM records r CROSS JOIN multiplier\n",
    "    \n",
    "    \"\"\"\n",
    "    bq.query(sql).result()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "import io,json\n",
    "def batch_load_table_from_json(js,table):\n",
    "    \"\"\" Does a batch (?) load from generated JSON\"\"\"\n",
    "    \n",
    "    table = bq.get_table(table)\n",
    "    bq.load_table_from_json(json_rows=js,destination=table)\n",
    "    \n",
    "\n",
    "def do_initial_load(*tables,n_records=100_000,update_rate=0.01):\n",
    "    \"\"\"\n",
    "    Performs an initial load of sample data. As generating huge amount of data can be slow, it generates some data and than simply multiplies it\n",
    "    \"\"\"\n",
    "    known_customers = []\n",
    "    \n",
    "    if update_rate > 0:\n",
    "        known_customers = get_customer_ids(int(n_records*update_rate))\n",
    "    \n",
    "    if len(known_customers) == 0 and update_rate > 0:\n",
    "        print(\"Tables are still empty. No known customers, hence no updates.\")\n",
    "        update_rate = 0\n",
    "        \n",
    "    max_records_in_ram = 100_000\n",
    "    num_chunks = math.ceil(n_records/max_records_in_ram)\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        print(f\"Initial Load of chunk {i} of size {max_records_in_ram} to target_tables {tables}\")\n",
    "        data = get_random_customers(n_records,for_streaming=False,update_rate=update_rate,known_customers=known_customers)\n",
    "        for table in tables:\n",
    "            batch_load_table_from_json(data,table)   # This is not blocking\n",
    "    \n",
    "\n",
    "import math\n",
    "\n",
    "def stream_into_tables(*tables, n_records=1_000_000,update_rate=0.01):\n",
    "    \"\"\"streams up to n_records into the target table(s)\"\"\"\n",
    "    if update_rate > 0:\n",
    "        known_customers = get_customer_ids(int(n_records*update_rate))\n",
    "    max_batch_size = 100\n",
    "    num_chunks = math.ceil(n_records/max_batch_size)\n",
    "    print(tables)\n",
    "    \n",
    "    for _ in range(num_chunks):\n",
    "        data = get_random_customers(max_batch_size,for_streaming=True,update_rate=update_rate,known_customers=known_customers)\n",
    "        \n",
    "        errors = Parallel(n_jobs=len(tables),require=\"sharedmem\")(delayed(streaming_insert)(data,table) for table in tables )\n",
    "        print(errors)\n",
    "        print(f\"Wrote batch of {max_batch_size} to tables {tables}\") \n",
    "\n",
    "import multiprocess\n",
    "from multiprocess import Pool  \n",
    "from functools import partial\n",
    "\n",
    "\n",
    "stream_to_table = partial(stream_into_tables, *target_tables, n_records=100_000,update_rate=0.55)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4350866a-06b3-487d-9698-d26bbbac811d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_latest_table():\n",
    "    \"\"\"\n",
    "    Refreshes the latest version table by recreating it. Handily it can also be used for generating the table for initially generating the table.\n",
    "    \"\"\"\n",
    "    refresh_latest_table =f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {dataset_name}.{table_names['latest_table']}\n",
    "    PARTITION BY DATE(ingest_ts)\n",
    "    AS\n",
    "    SELECT * FROM {dataset_name}.{table_names['streaming_table']}\n",
    "    QUALIFY ROW_NUMBER() OVER(PARTITION BY customer_id ORDER BY ingest_ts DESC) = 1\n",
    "    \"\"\"\n",
    "    \n",
    "    bq.query(refresh_latest_table).result()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b88765f-8761-46b5-9743-228aa65633fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_kappa_view():\n",
    "    \"\"\"\n",
    "    Creates the kappa view\n",
    "    \"\"\"\n",
    "    \n",
    "    sql = f\"\"\"\n",
    "    CREATE OR REPLACE VIEW {dataset_name}.{table_names['kappa_view']} AS\n",
    "        \n",
    " \n",
    "    WITH last_refresh_ts AS (\n",
    "        SELECT MAX(PARSE_DATE(\"%Y%m%d\", partition_id)) as ts\n",
    "         FROM  {dataset_name}.INFORMATION_SCHEMA.PARTITIONS\n",
    "         WHERE  1=1\n",
    "         AND table_name = '{table_names[\"latest_table\"]}'\n",
    "         AND partition_id <> '__STREAMING_UNPARTITIONED__'\n",
    "         AND total_rows > 0\n",
    "         ) ,\n",
    "   latest_table_data as (\n",
    "    SELECT *\n",
    "      FROM {dataset_name}.{table_names['latest_table']}\n",
    "    ),\n",
    "    latest_stream_data as (\n",
    "        SELECT *\n",
    "          FROM {dataset_name}.{table_names['streaming_table']}\n",
    "         WHERE DATE(ingest_ts) >= (SELECT ts               -- Uncorrelated Subquery\n",
    "                               FROM last_refresh_ts)\n",
    "    )\n",
    "    , combined_data as (\n",
    "        SELECT *\n",
    "         FROM latest_table_data\n",
    "        UNION ALL\n",
    "        SELECT *\n",
    "          FROM latest_stream_data\n",
    "    )\n",
    "    SELECT *\n",
    "      FROM combined_data\n",
    "    QUALIFY ROW_NUMBER() OVER(PARTITION BY customer_id\n",
    "                                  ORDER BY ingest_ts DESC)\n",
    "                         = 1\n",
    "        \"\"\"\n",
    "    bq.query(sql).result()\n",
    "\n",
    "\n",
    "\n",
    "def create_latest_version_view():\n",
    "    \"\"\"\n",
    "    Creates a latest version view (the view that determines the latest version for each entity on the fly based on the streaming table). This is the baseline \n",
    "    for our kappa view.\n",
    "    \"\"\"\n",
    "    \n",
    "    sql = f\"\"\"\n",
    "    CREATE OR REPLACE VIEW {dataset_name}.{table_names['latest_version_view']} AS \n",
    "    SELECT * \n",
    "    FROM {dataset_name}.{table_names['streaming_table']}\n",
    "    QUALIFY ROW_NUMBER() OVER(PARTITION BY customer_id ORDER BY ingest_ts DESC) = 1\n",
    "    \"\"\"\n",
    "    \n",
    "    bq.query(sql).result()\n",
    "    result = bq.query(sql).to_dataframe()\n",
    "    display(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "def query_latest_version_view():\n",
    "    \"\"\"Query the latest version view (just to see how much data it consumes)\"\"\"\n",
    "    \n",
    "    job_config = bigquery.QueryJobConfig(use_query_cache=False)    # For a fair comparison, do not use cached queries!\n",
    "\n",
    "    sql = f\"\"\"SELECT * FROM {dataset_name}.{table_names['latest_version_view']} \"\"\"\n",
    "    job = bq.query(sql,job_config=job_config)\n",
    "    job.result()\n",
    "    print(f\"Finished querying the latest version view. GB processed: {job.total_bytes_processed/1024**3} GB Billed {job.total_bytes_billed/1024**3}\")\n",
    "    return job.total_bytes_processed, job.total_bytes_billed\n",
    "\n",
    " \n",
    "def query_kappa_view():\n",
    "    \"\"\"Query the kappa view (just to see how much data it consumes)\"\"\"\n",
    "    \n",
    "    job_config = bigquery.QueryJobConfig(use_query_cache=False)    # For a fair comparison, do not use cached queries!\n",
    "    \n",
    "    sql = f\"\"\"SELECT * FROM {dataset_name}.{table_names['kappa_view']}\"\"\"\n",
    "    job = bq.query(sql,job_config=job_config)\n",
    "    job.result()\n",
    "    print(f\"Finished querying the kappa view. GB processed: {job.total_bytes_processed/1024**3} GB Billed {job.total_bytes_billed/1024**3}\")\n",
    "    return job.total_bytes_processed, job.total_bytes_billed\n",
    "\n",
    "def get_max_ingest_ts(table_name,ingest_ts_col_name=\"ingest_ts\"):\n",
    "    \"\"\"Queries the max ingest_ts of the table\"\"\"\n",
    "    sql = f\"\"\" SELECT MAX(INGEST_TS) as max_ts\n",
    "                FROM {table_name}\n",
    "           \"\"\"\n",
    "    job = bq.query(sql)\n",
    "    return list(job.result())[0][\"max_ts\"]\n",
    "    \n",
    "\n",
    "def get_dead_weight(table,pk_col=\"customer_id\", order_col=\"ingest_ts\"):\n",
    "    \"\"\"\n",
    "    Gets the 'dead weight' of a table (this is, it helps you calculating the ratio of current vs. history records. We consider history records dead weight.\n",
    "    \"\"\"\n",
    "    sql = f\"\"\"\n",
    "    WITH versions as \n",
    "    (SELECT  ROW_NUMBER() OVER (PARTITION BY {pk_col} ORDER BY {order_col}  DESC) = 1 AS is_latest_version_flag\n",
    "    FROM {table}\n",
    "    )\n",
    "    , agg as (\n",
    "    SELECT is_latest_version_flag, COUNT(*) as cnt\n",
    "    FROM versions\n",
    "    GROUP BY 1\n",
    "    )\n",
    "    SELECT * from agg\n",
    "    \"\"\"\n",
    "    \n",
    "    display(bq.query(sql).to_dataframe())\n",
    "    \n",
    "\n",
    "def get_partitions_for_table(table):\n",
    "    \"\"\"\n",
    "    Gets and displays the partition information for the table from the BQ INFORMATION_SCHEMA\n",
    "    \"\"\"\n",
    "    partitions = f\"\"\"\n",
    "    SELECT * FROM {dataset_name}.INFORMATION_SCHEMA.PARTITIONS\n",
    "    WHERE table_name = '{table}'\n",
    "    ORDER BY partition_id\n",
    "    \"\"\"\n",
    "\n",
    "    display(bq.query(partitions).to_dataframe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caf7a28-404e-4cb2-b932-7039815fade9",
   "metadata": {},
   "source": [
    "## Bulding Kappa\n",
    "Now let's put all pieces together: \n",
    "\n",
    "1. Create the streaming table\n",
    "2. Create the latest table\n",
    "3. Build the latest version and kappa view\n",
    "4. Generate some data\n",
    "5. Multiply the data to make the table huge\n",
    "\n",
    "This is going to take some minutes... so grab yourself another coffee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "349240bc-61c1-403d-b975-35f9a1526109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables are still empty. No known customers, hence no updates.Tables are still empty. No known customers, hence no updates.Tables are still empty. No known customers, hence no updates.\n",
      "\n",
      "\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "\n",
      "\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)Tables are still empty. No known customers, hence no updates.\n",
      "\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)Tables are still empty. No known customers, hence no updates.\n",
      "\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.Tables are still empty. No known customers, hence no updates.\n",
      "\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)Tables are still empty. No known customers, hence no updates.\n",
      "\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.Tables are still empty. No known customers, hence no updates.\n",
      "\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)Tables are still empty. No known customers, hence no updates.\n",
      "\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Tables are still empty. No known customers, hence no updates.Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "Tables are still empty. No known customers, hence no updates.\n",
      "Initial Load of chunk 0 of size 100000 to target_tables ('demo_kappa_architecture_final_2.customers_stream',)\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# Now let's put all together\n",
    "\n",
    "# Create Streaming Table\n",
    "create_streaming_table()\n",
    "# Create Latest Table\n",
    "refresh_latest_table()\n",
    "# Create Kappa View\n",
    "create_kappa_view()\n",
    "# Create Latest Table View\n",
    "create_latest_version_view()\n",
    "\n",
    "# Prepare initial load - put 20k records into the table\n",
    "records_per_load = 1_000\n",
    "initial_load_records = 100_000\n",
    "inital_load = partial(do_initial_load, *target_tables, n_records=records_per_load,update_rate=0.45)\n",
    "num_workers = 10\n",
    "\n",
    "from multiprocess import Pool\n",
    "with Pool(num_workers) as pool:\n",
    "    \n",
    "   pool_result = pool.starmap(inital_load, [() for _ in range(int(math.ceil(initial_load_records/records_per_load)))])\n",
    "\n",
    "# Lets multiply the number of records some time in order to make the table real big\n",
    "multiply_records_in_table(table_names[\"streaming_table\"],factor=10)   # will take us to 1,1 m records\n",
    "multiply_records_in_table(table_names[\"streaming_table\"],factor=10)  # 11m records\n",
    "multiply_records_in_table(table_names[\"streaming_table\"],factor=10)  # 110m records\n",
    "multiply_records_in_table(table_names[\"streaming_table\"],factor=5)  # 550m records\n",
    "\n",
    "#refresh the latest table after the initial load of the streaming table\n",
    "refresh_latest_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8996dfae-fa8e-49bd-b1de-2b405ddd4fa8",
   "metadata": {},
   "source": [
    "## Let the games begin\n",
    "Now that we have set everything up and our streaming table is populated, let the games begin... \n",
    "\n",
    "The initial load was to simulate some historic load of the table. Now let's start the streaming in order to simulate near-realtime provision of data. All the data that we insert from now on will not make it into the latest table (until we call refresh_latest_table method one more time). Contrary to the latest table, querying the kappa view will also return this brand-new data. \n",
    "\n",
    "The streaming runs in the background, so we can go on with our analysis in the meantime. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3971052-9b49-464c-a638-cd4c618822c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] running in background\n",
      "('demo_kappa_architecture_final_2.customers_stream',)\n",
      "('demo_kappa_architecture_final_2.customers_stream',)\n",
      "('demo_kappa_architecture_final_2.customers_stream',)\n",
      "('demo_kappa_architecture_final_2.customers_stream',)\n",
      "('demo_kappa_architecture_final_2.customers_stream',)\n",
      "[[]]\n",
      "Wrote batch of 100 to tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "[[]]\n",
      "Wrote batch of 100 to tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "[[]]\n",
      "Wrote batch of 100 to tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "[[]]\n",
      "Wrote batch of 100 to tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "[[]]\n",
      "Wrote batch of 100 to tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "[[]]\n",
      "Wrote batch of 100 to tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "[[]]\n",
      "Wrote batch of 100 to tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "[[]]\n",
      "[[]]Wrote batch of 100 to tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "\n",
      "Wrote batch of 100 to tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "[[]]\n",
      "Wrote batch of 100 to tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "[[]]\n",
      "Wrote batch of 100 to tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "[[]]\n",
      "Wrote batch of 100 to tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "[[]]\n",
      "Wrote batch of 100 to tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "[[]]\n",
      "Wrote batch of 100 to tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "[[]]\n",
      "Wrote batch of 100 to tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "[[]]\n",
      "Wrote batch of 100 to tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "[[]]\n",
      "Wrote batch of 100 to tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "[[]]\n",
      "Wrote batch of 100 to tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "[[]]\n",
      "Wrote batch of 100 to tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "[[]]\n",
      "Wrote batch of 100 to tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "[[]]\n",
      "Wrote batch of 100 to tables ('demo_kappa_architecture_final_2.customers_stream',)[[]]\n",
      "\n",
      "Wrote batch of 100 to tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "[[]]\n",
      "Wrote batch of 100 to tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "[[]]\n",
      "Wrote batch of 100 to tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "[[]]\n",
      "Wrote batch of 100 to tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "[[]]\n",
      "Wrote batch of 100 to tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "[[]]\n",
      "Wrote batch of 100 to tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "[[]]\n",
      "Wrote batch of 100 to tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "[[]]\n",
      "Wrote batch of 100 to tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "[[]]\n",
      "Wrote batch of 100 to tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "[[]]\n",
      "Wrote batch of 100 to tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "[[]]\n",
      "Wrote batch of 100 to tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "[[]]\n",
      "Wrote batch of 100 to tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "[[]]\n",
      "Wrote batch of 100 to tables ('demo_kappa_architecture_final_2.customers_stream',)\n",
      "[[]]\n",
      "Wrote batch of 100 to tables ('demo_kappa_architecture_final_2.customers_stream',)\n"
     ]
    }
   ],
   "source": [
    "%load_ext ipython_bg\n",
    "%%bg     # Need to use https://pypi.org/project/ipython-bg/  here - starmap_async is not working in jupyter notebooks\n",
    "\n",
    "def start_stream_in_background(num_workers=1):\n",
    "    \"\"\"\n",
    "    Stars a multiprocessing pool with n workers that all stream data into the table\n",
    "    \"\"\"\n",
    "    with Pool(num_workers) as pool:\n",
    "        # the map method takes a function / callable as first argument and a iterable which carries the argument for the callble as second. It chops the iterable into chunks, which it submits to the process pool. \n",
    "        # This works only for one-argumented function calls. For more arguments, you can use starmap-function instead. \n",
    "        # Note that this method is blocking - it returns only when all work items are finished. \n",
    "        # map_async instead is non-blocking and works with callbacks and AyncResult-Objects\n",
    "        results = pool.starmap(stream_to_table, [() for _ in range(10)])\n",
    "\n",
    "    \n",
    "start_stream_in_background(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76b97d2-7f67-4aec-b69c-254d964a40e0",
   "metadata": {},
   "source": [
    "## Examine the streaming table\n",
    "\n",
    "First let's have a look at the dead weight of the streaming table. The dead weight with respect to queries that care only about the latest version of an entity is the number of history versions in the table. When querying the table for all latest versions, these history versions are also read from the table and hence will be billed even though none of these records will make it into the result set. This is why for latest version queries, we consider these records as dead weight.\n",
    "\n",
    "Due to the way how we populated the table - by only generating a limited number of random records and multiplying it multiple times - we get a very bad ratio of latest versions to history versions (i.e. a high dead weight). This is a theoretical example. In real world, the dead weight of your streaming table depends on a number of factors like\n",
    "\n",
    "* Age / Lifetime of the table\n",
    "* Rate of updates per entity\n",
    "\n",
    "Most likely, you will never see such a heavy dead weight like here. This demo environment is made up for the kappa architecture and it can really shine here. Benefits in real world will not be that huge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d002fc1c-1f70-4e74-b328-7ae480f345f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_latest_version_flag</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>100252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>False</td>\n",
       "      <td>776720248</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_latest_version_flag        cnt\n",
       "0                    True     100252\n",
       "1                   False  776720248"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's have a look at the dead weight\n",
    "get_dead_weight(*target_tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5556d610-dd26-476b-9b56-ce5150ae09df",
   "metadata": {},
   "source": [
    "## Run the queries and compare\n",
    "Let's wait some minutes until the streaming processes inserted some records. Then let's see if the kappa view returns not only the already materialized data from latest table, but also those records that have lately been inserted to the streaming table by the streaming process running in background. Then let's query both the latest record view and the kappa view and compare the data processed / query costs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10cfd23-538c-4840-8ea3-1af818f73c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import timezone\n",
    "\n",
    "waiting_sec = 60 * 5   # 5 minutes\n",
    "time.sleep(waiting_sec)\n",
    "\n",
    "# Lets check the max ingest_ts from kappa view as indicator whether kappa view also returns the latest streaming data\n",
    "max_ingest_ts = get_max_ingest_ts(target_tables[0])\n",
    "print(f\"Max ingest_ts in kappa view: {max_ingest_ts}\")\n",
    "assert (datetime.datetime.now(timezone.utc) - max_ingest_ts) < datetime.timedelta(minutes=3)\n",
    "\n",
    "latest_view_data = query_latest_version_view()\n",
    "kappa_view_data = query_kappa_view()\n",
    "print()\n",
    "print()\n",
    "print(f\"Kappa view consumes {kappa_view_data[0]/latest_view_data[0] * 100} % of the data that the latest view consumes. Quot erat demonstrandum!\")\n",
    "print()\n",
    "print(\"But let's be honest here. We need to mention that due to our data multiplication strategy the streaming table has a very bad latest to history version ratio. \\\n",
    "You are not going to find anything comparable in real world. But also when your streaming table has less dead weight, you will be able to benefit from kappa architecture!\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m93"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
